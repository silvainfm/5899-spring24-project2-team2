{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "779d9b41",
   "metadata": {},
   "source": [
    "### With Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37fb3090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sruthi/anaconda3/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Define keywords for events\n",
    "event_keywords = {\n",
    "    'default': ['bankruptcy', 'default', 'insolvency'],\n",
    "    'mergers_acquisitions': ['merger', 'acquisition', 'takeover'],\n",
    "    'revenue': ['revenue', 'sales', 'earnings'],\n",
    "    'margin_profitability': ['margin', 'profitability', 'operating income'],\n",
    "    'industry_competition': ['competition', 'market share', 'competitor']\n",
    "}\n",
    "\n",
    "# Check if event is mentioned in the text\n",
    "def is_event_mentioned(text, keywords):\n",
    "    for word in keywords:\n",
    "        if word in text.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Function to compute sentiment score\n",
    "def get_sentiment_score(text, tokenizer, model, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = softmax(logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    sentiment_score = probabilities[0, 2] - probabilities[0, 0]\n",
    "    return sentiment_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('updated_final_annotated_dataset_with_impacts.csv')\n",
    "\n",
    "# Apply the event checking function to each event type and create a new column for it\n",
    "for event_type, keywords in event_keywords.items():\n",
    "    column_name = f'{event_type}_mentioned'\n",
    "    df[column_name] = df['content'].apply(lambda text: is_event_mentioned(text, keywords))\n",
    "\n",
    "# Filter the impact scores based on whether the corresponding event is mentioned\n",
    "for event_type in event_keywords:\n",
    "    impact_column = f'{event_type}_impact'\n",
    "    mentioned_column = f'{event_type}_mentioned'\n",
    "    df[impact_column] = df.apply(lambda row: row[impact_column] if row[mentioned_column] else 'no_event', axis=1)\n",
    "\n",
    "# Combine the filtered impact scores into a single column\n",
    "df['filtered_impacts'] = df[[f'{event_type}_impact' for event_type in event_keywords]].apply(lambda row: ' '.join(str(val) for val in row.values), axis=1)\n",
    "\n",
    "# Map the categorical labels to integers\n",
    "label_to_id = {'good': 1, 'neutral': 0, 'bad': -1, 'no_event': 0}\n",
    "df['impact_numerical'] = df['filtered_impacts'].apply(lambda impacts: max([label_to_id.get(impact, 0) for impact in impacts.split()]))\n",
    "\n",
    "# Load tokenizer and model for sentiment analysis\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "model = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Compute the sentiment score for each article and add it as a column\n",
    "df['sentiment_score'] = df['content'].apply(lambda text: get_sentiment_score(text, tokenizer, model, device))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "725191ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sruthi/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed\n",
      "Epoch 2 completed\n",
      "Epoch 3 completed\n",
      "Test accuracy: 0.8741\n"
     ]
    }
   ],
   "source": [
    "# Specify the column names\n",
    "text_column = 'content'\n",
    "true_label_column = 'impact_numerical'\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[text_column], df[true_label_column], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the training and test data\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Convert labels to numpy and then to tensors\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(train_labels.values)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(test_encodings['input_ids']),\n",
    "    torch.tensor(test_encodings['attention_mask']),\n",
    "    torch.tensor(test_labels.values)\n",
    ")\n",
    "\n",
    "# DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Training loop\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed\")\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(dataloader, model):\n",
    "    model.eval()\n",
    "    total_accuracy = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            accuracy = (preds == labels).float().mean()\n",
    "            total_accuracy += accuracy.item()\n",
    "\n",
    "    return total_accuracy / len(dataloader)\n",
    "\n",
    "test_accuracy = evaluate_model(test_dataloader, model)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4abc640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>summary</th>\n",
       "      <th>description</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Company</th>\n",
       "      <th>pubDate_brief</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>categories</th>\n",
       "      <th>...</th>\n",
       "      <th>margin_profitability_impact</th>\n",
       "      <th>industry_competition_impact</th>\n",
       "      <th>default_mentioned</th>\n",
       "      <th>mergers_acquisitions_mentioned</th>\n",
       "      <th>revenue_mentioned</th>\n",
       "      <th>margin_profitability_mentioned</th>\n",
       "      <th>industry_competition_mentioned</th>\n",
       "      <th>filtered_impacts</th>\n",
       "      <th>impact_numerical</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12024</td>\n",
       "      <td>Osaka Governor Hirofumi Yoshimura said that th...</td>\n",
       "      <td>Years of delay to plans for Japan‚Äö√Ñ√¥s firs...</td>\n",
       "      <td>MGM</td>\n",
       "      <td>Services</td>\n",
       "      <td>Casinos &amp; Gaming</td>\n",
       "      <td>MGM Resorts International</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023-05-18T21:25:29+00:00</td>\n",
       "      <td>[{'name': 'Health'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>no_event no_event no_event no_event no_event</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.999780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20675</td>\n",
       "      <td>MetLife (MET) is a Finance stock that has seen...</td>\n",
       "      <td>Dividends are one of the best benefits to bein...</td>\n",
       "      <td>MET</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Insurance</td>\n",
       "      <td>Metlife Inc</td>\n",
       "      <td>2022-10-31</td>\n",
       "      <td>2022-10-31T20:36:25+00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>no_event</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>no_event no_event good no_event good</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.999844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33685</td>\n",
       "      <td>This week, top-five producer AngloGold Ashanti...</td>\n",
       "      <td>(Bloomberg) -- The momentum has been building ...</td>\n",
       "      <td>NEM</td>\n",
       "      <td>Extractives &amp; Minerals Processing</td>\n",
       "      <td>Metals &amp; Mining</td>\n",
       "      <td>Newmont Corp</td>\n",
       "      <td>2023-02-08</td>\n",
       "      <td>2023-02-08T22:16:21+00:00</td>\n",
       "      <td>[{'name': 'Politics'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>no_event no_event no_event no_event no_event</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12072</td>\n",
       "      <td>The case is In re Tesla Inc Securities Litigat...</td>\n",
       "      <td>Some of the biggest securities cases of 2023 a...</td>\n",
       "      <td>NDAQ</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Security &amp; Commodity Exchanges</td>\n",
       "      <td>Nasdaq Inc</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>2023-05-18T14:28:52+00:00</td>\n",
       "      <td>[{'name': 'Tech'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>no_event</td>\n",
       "      <td>no_event</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>no_event no_event no_event no_event no_event</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.882581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28164</td>\n",
       "      <td>CFOs Boost Currency Protections, Extend Hedge ...</td>\n",
       "      <td>Coca-Cola, Kimberly-Clark and Prologis are amo...</td>\n",
       "      <td>KO</td>\n",
       "      <td>Food &amp; Beverage</td>\n",
       "      <td>Non-Alcoholic Beverages</td>\n",
       "      <td>Coca-Cola Co</td>\n",
       "      <td>2023-05-04</td>\n",
       "      <td>2023-05-04T23:39:33+00:00</td>\n",
       "      <td>[{'name': 'Tech'}]</td>\n",
       "      <td>...</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>no_event good good good good</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            summary  \\\n",
       "0  12024  Osaka Governor Hirofumi Yoshimura said that th...   \n",
       "1  20675  MetLife (MET) is a Finance stock that has seen...   \n",
       "2  33685  This week, top-five producer AngloGold Ashanti...   \n",
       "3  12072  The case is In re Tesla Inc Securities Litigat...   \n",
       "4  28164  CFOs Boost Currency Protections, Extend Hedge ...   \n",
       "\n",
       "                                         description Ticker  \\\n",
       "0  Years of delay to plans for Japan‚Äö√Ñ√¥s firs...    MGM   \n",
       "1  Dividends are one of the best benefits to bein...    MET   \n",
       "2  (Bloomberg) -- The momentum has been building ...    NEM   \n",
       "3  Some of the biggest securities cases of 2023 a...   NDAQ   \n",
       "4  Coca-Cola, Kimberly-Clark and Prologis are amo...     KO   \n",
       "\n",
       "                              Sector                        Industry  \\\n",
       "0                           Services                Casinos & Gaming   \n",
       "1                         Financials                       Insurance   \n",
       "2  Extractives & Minerals Processing                 Metals & Mining   \n",
       "3                         Financials  Security & Commodity Exchanges   \n",
       "4                    Food & Beverage         Non-Alcoholic Beverages   \n",
       "\n",
       "                     Company pubDate_brief                    pubDate  \\\n",
       "0  MGM Resorts International    2023-05-18  2023-05-18T21:25:29+00:00   \n",
       "1                Metlife Inc    2022-10-31  2022-10-31T20:36:25+00:00   \n",
       "2               Newmont Corp    2023-02-08  2023-02-08T22:16:21+00:00   \n",
       "3                 Nasdaq Inc    2023-05-18  2023-05-18T14:28:52+00:00   \n",
       "4               Coca-Cola Co    2023-05-04  2023-05-04T23:39:33+00:00   \n",
       "\n",
       "               categories  ... margin_profitability_impact  \\\n",
       "0    [{'name': 'Health'}]  ...                    no_event   \n",
       "1                      []  ...                    no_event   \n",
       "2  [{'name': 'Politics'}]  ...                    no_event   \n",
       "3      [{'name': 'Tech'}]  ...                    no_event   \n",
       "4      [{'name': 'Tech'}]  ...                        good   \n",
       "\n",
       "  industry_competition_impact default_mentioned  \\\n",
       "0                    no_event             False   \n",
       "1                        good             False   \n",
       "2                    no_event             False   \n",
       "3                    no_event             False   \n",
       "4                        good             False   \n",
       "\n",
       "   mergers_acquisitions_mentioned  revenue_mentioned  \\\n",
       "0                           False              False   \n",
       "1                           False               True   \n",
       "2                           False              False   \n",
       "3                           False              False   \n",
       "4                            True               True   \n",
       "\n",
       "   margin_profitability_mentioned  industry_competition_mentioned  \\\n",
       "0                           False                           False   \n",
       "1                           False                            True   \n",
       "2                           False                           False   \n",
       "3                           False                           False   \n",
       "4                            True                            True   \n",
       "\n",
       "                               filtered_impacts impact_numerical  \\\n",
       "0  no_event no_event no_event no_event no_event                0   \n",
       "1          no_event no_event good no_event good                1   \n",
       "2  no_event no_event no_event no_event no_event                0   \n",
       "3  no_event no_event no_event no_event no_event                0   \n",
       "4                  no_event good good good good                1   \n",
       "\n",
       "   sentiment_score  \n",
       "0        -0.999780  \n",
       "1        -0.999844  \n",
       "2         0.985379  \n",
       "3        -0.882581  \n",
       "4         0.000061  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6ac565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been saved to 'modified_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "df.to_csv('modified_dataset.csv', index=False)\n",
    "\n",
    "print(\"Dataset has been saved to 'modified_dataset.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a14cbe8",
   "metadata": {},
   "source": [
    "# Separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14fc7900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset for default_present saved with shape: (165, 24) to default_present_subset.csv\n",
      "Subset for mergers_acquisitions_present saved with shape: (269, 24) to mergers_acquisitions_present_subset.csv\n",
      "Subset for revenue_present saved with shape: (1276, 24) to revenue_present_subset.csv\n",
      "Subset for margin_profitability_present saved with shape: (379, 24) to margin_profitability_present_subset.csv\n",
      "Subset for industry_competition_present saved with shape: (622, 24) to industry_competition_present_subset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('modified_dataset.csv')\n",
    "\n",
    "# Define impact types and their related columns\n",
    "impact_details = {\n",
    "    'default_present': ['default_present', 'default_impact', 'default_mentioned', 'default_sentiment'],\n",
    "    'mergers_acquisitions_present': ['mergers_acquisitions_present', 'mergers_acquisitions_impact', 'mergers_acquisitions_mentioned', 'mergers_acquisitions_sentiment'],\n",
    "    'revenue_present': ['revenue_present', 'revenue_impact', 'revenue_mentioned', 'revenue_sentiment'],\n",
    "    'margin_profitability_present': ['margin_profitability_present', 'margin_profitability_impact', 'margin_profitability_mentioned', 'margin_profitability_sentiment'],\n",
    "    'industry_competition_present': ['industry_competition_present', 'industry_competition_impact', 'industry_competition_mentioned', 'industry_competition_sentiment']\n",
    "}\n",
    "\n",
    "# Iterate over each impact type and filter the dataset where the impact is marked as present\n",
    "for impact_type, columns in impact_details.items():\n",
    "    subset = data[data[impact_type] == True]  # Filter where impact is present\n",
    "\n",
    "    # Select only relevant columns for this impact type\n",
    "    relevant_columns = columns + ['index', 'summary', 'description', 'Ticker', 'Sector', 'Industry',\n",
    "                                  'Company','pubDate', 'pubDate_brief', 'pubDate', 'categories', 'content', 'title',\n",
    "                                  'relationship_type', 'financial_entities', 'extracted_tickers_summary',\n",
    "                                  'extracted_tickers_description', 'extracted_tickers_financial_entities',\n",
    "                                  'relevant_companies','sentiment_score']  # Add any other general columns needed\n",
    "\n",
    "    subset = subset[relevant_columns]\n",
    "    subset = subset.dropna(subset=['Ticker'])\n",
    "\n",
    "    # Save the filtered and trimmed dataset to a CSV file\n",
    "    filename = f'{impact_type}_subset.csv'\n",
    "    subset.to_csv(filename, index=False)\n",
    "    print(f\"Subset for {impact_type} saved with shape: {subset.shape} to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a39ad1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'default_impact': ['no_event' nan 'good' 'bad']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('modified_dataset.csv')\n",
    "\n",
    "# Print unique values in the 'default_impact' column\n",
    "print(\"Unique values in 'default_impact':\", data['default_impact'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be3ceb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_mapping = {'no_event': 0, 'good': 1, 'bad': 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ba13e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_format(examples):\n",
    "    # Tokenize the text\n",
    "    result = tokenizer(examples['summary'], padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # Add the label using the defined mapping\n",
    "    # Here we're using 'default_impact' and a mapping that needs to be defined based on actual column values\n",
    "    impact_mapping = {'no_event': 0, 'good': 1, 'bad': 2}  # Update this mapping based on actual data\n",
    "    result['labels'] = [impact_mapping[label] for label in examples['default_impact']]\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d2230cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'default_impact': ['no_event' nan 'good' 'bad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset for impact type: default\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e4e44c897547a39200ab622068e80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sruthi/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 06:40, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.319026</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.110800</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/default/checkpoint-7 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./results/default/checkpoint-15 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./results/default/checkpoint-21 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset for impact type: mergers_acquisitions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f21d07e245049beb7a37465a0718b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sruthi/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 27:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.657766</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.499346</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.436659</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset for impact type: revenue\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90504c0e0f2c482184567e87a598b8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1181 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sruthi/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='177' max='177' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [177/177 2:05:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.146829</td>\n",
       "      <td>0.970464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.140352</td>\n",
       "      <td>0.970464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.144813</td>\n",
       "      <td>0.970464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset for impact type: margin_profitability\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02148f9ddb664f0580b34a096b55a1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sruthi/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 8:11:33, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.254508</td>\n",
       "      <td>0.942029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.268567</td>\n",
       "      <td>0.942029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset for impact type: industry_competition\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cd5316d9ff43078ab0efc381bfcbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sruthi/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 1:12:56, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.082936</td>\n",
       "      <td>0.988889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.067873</td>\n",
       "      <td>0.988889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('modified_dataset.csv')\n",
    "\n",
    "# Checking unique values before proceeding\n",
    "print(\"Unique values in 'default_impact':\", data['default_impact'].unique())\n",
    "\n",
    "# Define impact types based on your dataset columns\n",
    "impact_types = ['default', 'mergers_acquisitions', 'revenue', 'margin_profitability', 'industry_competition']\n",
    "\n",
    "# Split the dataset by impact type using the '_impact' suffix\n",
    "impact_datasets = {}\n",
    "for impact in impact_types:\n",
    "    impact_datasets[impact] = data[data[f'{impact}_impact'] == 'good']  # Assuming 'good' indicates presence\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # Example: 3 labels\n",
    "\n",
    "def tokenize_and_format(examples):\n",
    "    result = tokenizer(examples['summary'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    impact_mapping = {'no_event': 0, 'good': 1, 'bad': 2}\n",
    "    result['labels'] = [impact_mapping.get(label, 0) for label in examples['default_impact']]\n",
    "    return result\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# Process each dataset with updated tokenization and labeling\n",
    "for impact, df in impact_datasets.items():\n",
    "    print(f\"Processing dataset for impact type: {impact}\")\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    tokenized_dataset = hf_dataset.map(tokenize_and_format, batched=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results/{impact}',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset.select([i for i in range(int(len(tokenized_dataset) * 0.8))]),\n",
    "        eval_dataset=tokenized_dataset.select([i for i in range(int(len(tokenized_dataset) * 0.8), len(tokenized_dataset))]),\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be6e69a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries for default impact: 149\n"
     ]
    }
   ],
   "source": [
    "# Example of checking one impact type\n",
    "impact_type = 'default'\n",
    "filtered_data = data[data[f'{impact_type}_impact'] == 'good']  # Assuming 'good' indicates presence\n",
    "print(f\"Entries for {impact_type} impact:\", len(filtered_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52fe2a06",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'default_present_impact'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'default_present_impact'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Process each dataset with updated tokenization and labeling\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impact \u001b[38;5;129;01min\u001b[39;00m impact_types:\n\u001b[0;32m----> 3\u001b[0m     subset \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimpact\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_impact\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgood\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subset\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo entries found for impact type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimpact\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'default_present_impact'"
     ]
    }
   ],
   "source": [
    "# Process each dataset with updated tokenization and labeling\n",
    "for impact in impact_types:\n",
    "    subset = data[data[f'{impact}_impact'] == 'good']\n",
    "    if subset.empty:\n",
    "        print(f\"No entries found for impact type: {impact}\")\n",
    "    else:\n",
    "        print(f\"Processing {len(subset)} entries for impact type: {impact}\")\n",
    "        hf_dataset = Dataset.from_pandas(subset)\n",
    "        tokenized_dataset = hf_dataset.map(tokenize_and_format, batched=True)\n",
    "\n",
    "        # Check if tokenized dataset is correct\n",
    "        print(\"Sample tokenized data:\", tokenized_dataset[:2])\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'./results/{impact}',\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            gradient_accumulation_steps=2,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\"\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_dataset.select([i for i in range(int(len(tokenized_dataset) * 0.8))]),\n",
    "            eval_dataset=tokenized_dataset.select([i for i in range(int(len(tokenized_dataset) * 0.8), len(tokenized_dataset))]),\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        # Train the model and capture metrics\n",
    "        train_result = trainer.train()\n",
    "        eval_result = trainer.evaluate()\n",
    "\n",
    "        # Store the results\n",
    "        print(\"Training Loss:\", train_result.training_loss)\n",
    "        print(\"Evaluation Results:\", eval_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f29db79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the dataset: Index(['index', 'summary', 'description', 'Ticker', 'Sector', 'Industry',\n",
      "       'Company', 'pubDate_brief', 'pubDate', 'categories', 'content', 'title',\n",
      "       'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15',\n",
      "       'Unnamed: 16', 'default_present', 'default_sentiment',\n",
      "       'mergers_acquisitions_present', 'mergers_acquisitions_sentiment',\n",
      "       'revenue_present', 'revenue_sentiment', 'margin_profitability_present',\n",
      "       'margin_profitability_sentiment', 'industry_competition_present',\n",
      "       'industry_competition_sentiment', 'relationship_type',\n",
      "       'financial_entities', 'extracted_tickers_summary',\n",
      "       'extracted_tickers_description', 'extracted_tickers_financial_entities',\n",
      "       'relevant_companies', 'default_impact', 'mergers_acquisitions_impact',\n",
      "       'revenue_impact', 'margin_profitability_impact',\n",
      "       'industry_competition_impact', 'default_mentioned',\n",
      "       'mergers_acquisitions_mentioned', 'revenue_mentioned',\n",
      "       'margin_profitability_mentioned', 'industry_competition_mentioned',\n",
      "       'filtered_impacts', 'impact_numerical', 'sentiment_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('modified_dataset.csv')\n",
    "\n",
    "# Print column names to identify impact type columns\n",
    "print(\"Column names in the dataset:\", data.columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
